<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta charset="UTF-8">
	<!-- Global Site Tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112301535-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-112301535-1');
	</script>
	  <meta name=viewport content=‚Äúwidth=800‚Äù>
	  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
	  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    li:not(:last-child) {
        margin-bottom: -10px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }

    img {
      border-radius: 0px;
    }
  </style>
  <link rel="icon" type="image/png" href="psihijatar-footer-1.png">
  <title>Karl Pertsch</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Yuxuan Chen (ÈôàÂÆáËΩ©)</name>
              </p>
              <p>I'm a first-year Ph.D. student from School of Mechanical Engineering, Shanghai Jiao Tong University. I am currently advised by Prof. <a target="_blank" href="https://me.sjtu.edu.cn/teacher_directory1/lixiao.html">Xiao Li</a>. My research interests include efficient robotic policies, large language models, and embodied AI. </p>
              </p>
              <p align=center>
                <a href="chen_yuxuan@sjtu.edu.cn">Email</a> &nbsp/&nbsp
                <a target="_blank" href="https://scholar.google.cz/citations?user=LEjLVY8AAAAJ&hl=zh-CN&oi=sra">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://github.com/Elycyx">GitHub</a>
              </p>
            </td>
            <td width="33%">
              <img src="resources/avater.png" width="250" height="250">
            </td>
          </tr>
        </table>

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	      <tr>
	        <td width="100%" valign="middle">
	          <heading>News</heading>
	          <p>
	            <ul>
                <li> [Oct 2022] Our work on  <a target="_blank" href="https://kpertsch.github.io/star">cross-domain imitation learning</a> got accepted to CoRL'22! </li> <br>
                <li> [Mar 2022] Two papers accepted to ICLR 2022: <a target="_blank" href="https://clvrai.com/tarp">Task-Induced Representation Learning</a> and <a target="_blank" href="https://clvrai.com/simpl">Skill-based Meta-Reinforcement Learning</a>! </li> <br>
	             <li> [Sept 2021] Our <a target="_blank" href="https://arxiv.org/abs/2107.10253">SkiLD paper</a> will be presented at CoRL 2021! </li> <br>
                <li> [Jul 2021] New <a target="_blank" href="https://arxiv.org/abs/2107.10253">preprint</a> on skill-based learning with demonstrations! </li> <br>
                <li> [Jun 2021] I presented our work on skill-based reinforcement & imitation learning in the <a target="_blank" href=https://www.seas.upenn.edu/~dineshj/pal/index.html">PAL Lab</a> at UPenn and in the <a target="_blank" href=http://svl.stanford.edu/">Stanford Vision & Learning Lab</a>. Check the <a target="_blank" href="https://drive.google.com/file/d/14xn9ojYfv8rSxVf5fPTixnkTpJRWUKKg/view?usp=sharing">Slides here</a>!</li><br>
                <li> [Dec 2020] Received the CoRL 2020 <span style="color: #ff0000">Best Paper Presentation Award</span> for our <a target="_blank" href="https://arxiv.org/abs/2010.11944">SPiRL</a> paper, check out the <a target="_blank" href="https://youtu.be/kZOcqFRj5NE?t=5119">talk recording</a>!</li><br>
	              <li> [Nov 2020] <a target="_blank" href="https://arxiv.org/abs/2010.11944">SPiRL</a> will be presented as a <span style="color: #ff0000">plenary talk</span> at CoRL & won the <span style="color: #ff0000">best paper runner-up award</span> at the robot learning workshop @ NeurIPS!</li><br>
                <li> [Oct 2020] Two papers accepted to CoRL 2020 (<a target="_blank" href="https://arxiv.org/abs/2010.11944">SPiRL</a> and <a target="_blank" href="https://arxiv.org/abs/2010.11940">MoPA-RL</a>)! </li> <br> -->
                <!-- <li> [Sept 2020] Our hierarchical prediction and planning paper was accepted to NeurIPS2020!</li> <br> -->
                <!-- <li> [Jun 2020] New <a target="_blank" href="https://arxiv.org/abs/2006.13205">preprint</a> on long-horizon visual planning using hierarchical prediction! </li> <br> -->
	              <!-- <li> [Apr 2020] Our work on keyframe-based prediction will be presented at L4DC'20!</li> <br> -->
                <!-- <li> [Apr 2019] New <a target="_blank" href="https://arxiv.org/abs/1904.05869">preprint</a> on keyframe-based video prediction! </li> <br> -->
                <!-- <li> [Apr 2019] Our work on discovering an agent's action space got accepted to ICLR19! </li> <br> -->
                <!--<li> [Dec 2018] I presented our work on unsupervised learning of agent's action spaces at the <a target="_blank" href="https://sites.google.com/view/infer2control-nips2018">Infer2Control workshop</a> at NeurIPS 2018 in Montreal. </li> <br> -->
                <!-- <br> -->
	              <!-- <li> [Aug 2017] Starting my one year Fulbright research stay in <a target="_blank" href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> group at UPenn. </li> <br> -->
	            <!-- </ul>  
	          </p>
	        </td>
	      </tr>
      	</table> -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>üéì Education</heading>
            </td>
          </tr>
        </table>
        <hr>

        <table id="tbPublications" width="100%">
          <tbody>
          <tr>
            <td width="320">
            <img src="resources/sjtu.png" width="120px" style="margin-left: 20px">
            </td>				
            <td>
              <p><b>Shanghai Jiao Tong University (SJTU)</b><br><b style="font-size: 12px;">School of Mechanical Engineering</b></p>
              <div style="font-size: 12px">Ph.D. Student: 2025.09 - Present, supervised by Prof. <a href="https://me.sjtu.edu.cn/teacher_directory1/lixiao.html">Xiao Li</a>.</div>
            </td>
          </tr>
          </tbody>
        </table>

        <table id="tbPublications" width="100%">
          <tbody>
          <tr>
            <td width="320">
            <img src="resources/sjtu.png" width="120px" style="margin-left: 20px">
            </td>				
            <td>
              <p><b>Shanghai Jiao Tong University (SJTU)</b><br><b style="font-size: 12px;">School of Mechanical Engineering</b><br><b style="font-size: 12px;">Major in Mechanical Engineering</b></p>
              <div style="font-size: 12px">Undergraduate Student: 2021.09 - 2025.06.</div>
            </td>
          </tr>
          </tbody>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>üìù Publications</heading>
            </td>
          </tr>
        </table>
        <hr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tr  onmouseout="_start()" onmouseover="rlrc_start()">
          <td width="25%">
            <div class="one">
            <img src='resources/rlrc.png' width="180" height="100" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function rlrc_start() {
            document.getElementById('rlrc_image').style.opacity = "1";
            }
            function rlrc_stop() {
            document.getElementById('rlrc_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://rlrc-vla.github.io/">
            <papertitle>RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models</papertitle>
            </a>
            <br>
            <strong>Yuxuan Chen</strong>, 
            Xiao Li<br>
            <i>ArXiv</i>, 2506.17639<br>
            <a target="_blank" href ="https://arxiv.org/abs/2506.17639">paper</a>  / 
            <a target="_blank" href ="https://rlrc-vla.github.io/">website</a>  / 
            <br>
          </p>
          <p>We propose RLRC, a three-stage recovery method for compressed VLAs, including structured pruning, performance recovery based on SFT and RL, and further quantization. RLRC achieves up to an 8x reduction in memory usage and a 2.3x improvement in inference throughput, while maintaining or even surpassing the original VLA's task success rate. </p>
          <p>
          </p>  
          </td>
        </tr>

        <tr  onmouseout="_start()" onmouseover="fastnav_start()">
          <td width="25%">
            <div class="one">
            <!--<div class="two" id = 'fastnav_image'><img src='resources/fastnav.gif' width="160" height="160" style="background-color:white;"></div>-->
            <img src='resources/fastnav.png' width="180" height="100" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function fastnav_start() {
            document.getElementById('fastnav_image').style.opacity = "1";
            }
            function fastnav_stop() {
            document.getElementById('fastnav_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/2408.14037">
            <papertitle>FASTNav: Fine-Tuned Adaptive Small-Language-Models Trained for Multi-Point Robot Navigation</papertitle>
            </a>
            <br>
            <strong>Yuxuan Chen</strong>, 
            Yixin Han,
            Xiao Li<br>
            <i>IEEE Robotics and Automation Letters (RA-L)</i>, 2024<br>
            <a target="_blank" href ="https://arxiv.org/abs/2411.13262">paper</a>
            <br>
          </p>
          <p> We propose FASTNav - a method for boosting lightweight LLMs, also known as small language models (SLMs), for robot navigation. The proposed method contains three modules: fine-tuning, teacher-student iteration, and language-based multi-point robot navigation. We train and evaluate models with FASTNav in both simulation and real robots, proving that we can deploy them with low cost, high accuracy and low response time. </p>
          <p>
          </p>  
          </td>
        </tr>

        </table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	      <tr>
	        <td>
	        <br>
	        <p align="right">
	          <font size="2">
	          <strong>I borrowed this website layout from <a target="_blank" href="https://jonbarron.info/">here</a>!</strong>
		    </font>
	        </p>
	        </td>
	      </tr>
      	</table>

        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
