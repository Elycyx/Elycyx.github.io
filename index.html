<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
	<!-- Global Site Tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112301535-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-112301535-1');
	</script>
	  <meta name=viewport content=“width=800”>
	  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
	  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    li:not(:last-child) {
        margin-bottom: -10px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }

    img {
      border-radius: 15px;
    }
  </style>
  <link rel="icon" type="image/png" href="psihijatar-footer-1.png">
  <title>Karl Pertsch</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Karl Pertsch</name>
              </p>
              <p>I am a PhD student in the <a target="_blank" href="http://clvrai.com/">Cognitive Learning for Vision and Robotics Lab (CLVR)</a> at the University of Southern California where I work on deep learning, reinforcement learning and robotics with Professor <a target="_blank" href="http://www-bcf.usc.edu/~limjj/">Joseph Lim</a>. 
              </p>
              <p>
                Before joining CLVR I obtained my diploma in EE from TU Dresden, Germany working with Professor <a target="_blank" href="https://hci.iwr.uni-heidelberg.de/vislearn/people/carsten-rother/">Carsten Rother</a>. I also got the chance to spend one year as a Fulbright Scholar in the <a target="_blank" href="https://www.grasp.upenn.edu/">GRASP Lab</a> at the University of Pennsylvania working with Professor <a target="_blank" href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>. In Spring 2019 I spent four months at UC Berkeley working with Professor <a target="_blank" href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>.
              </p>
              <p align=center>
                <a href="mailto:pertsch@usc.edu">Email</a> &nbsp/&nbsp
                <a target="_blank" href="https://twitter.com/KarlPertsch"> Twitter </a> &nbsp/&nbsp
                <a target="_blank" href="https://scholar.google.com/citations?view_op=list_works&hl=en&user=3oe0I0QAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="resume/resume_pertsch.pdf">CV</a> &nbsp/&nbsp
                <a target="_blank" href="https://www.linkedin.com/in/karlpertsch/"> LinkedIn </a>
              </p>
            </td>
            <td width="33%">
              <img src="profile_karl.jpg" width="250" height="250">
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	      <tr>
	        <td width="100%" valign="middle">
	          <heading>News</heading>
	          <p>
	            <ul>
	            <li> [Nov 2020] <a target="_blank" href="https://arxiv.org/abs/2010.11944">SPiRL</a> will be presented as a <span style="color: #ff0000">plenary talk</span> at CoRL & won the <span style="color: #ff0000">best paper runner-up award</span> at the robot learning workshop @ NeurIPS!</li><br>
                <li> [Oct 2020] Two papers accepted to CoRL 2020 (<a target="_blank" href="https://arxiv.org/abs/2010.11944">SPiRL</a> and <a target="_blank" href="https://arxiv.org/abs/2010.11940">MoPA-RL</a>)! </li> <br>
                <li> [Sept 2020] Our hierarchical prediction and planning paper was accepted to NeurIPS2020!</li> <br>
                <li> [Jun 2020] New <a target="_blank" href="https://arxiv.org/abs/2006.13205">preprint</a> on long-horizon visual planning using hierarchical prediction! </li> <br>
	              <li> [Apr 2020] Our work on keyframe-based prediction will be presented at L4DC'20!</li> <br>
                <!-- <li> [Apr 2019] New <a target="_blank" href="https://arxiv.org/abs/1904.05869">preprint</a> on keyframe-based video prediction! </li> <br> -->
                <!-- <li> [Apr 2019] Our work on discovering an agent's action space got accepted to ICLR19! </li> <br> -->
                <!--<li> [Dec 2018] I presented our work on unsupervised learning of agent's action spaces at the <a target="_blank" href="https://sites.google.com/view/infer2control-nips2018">Infer2Control workshop</a> at NeurIPS 2018 in Montreal. </li> <br> -->
                <br>
	              <!-- <li> [Aug 2017] Starting my one year Fulbright research stay in <a target="_blank" href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> group at UPenn. </li> <br> -->
	            </ul>  
	          </p>
	        </td>
	      </tr>
      	</table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                I'm interested in machine learning, reinforcement learning and robotics. At the moment, I am working on methods for using large datasets to facilitate the learning of skills and behaviors. In particular, I focus on learning predictive models for planning and imitation learning from unstructured datasets.
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tr  onmouseout="spirl_start()" onmouseover="spirl_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'spirl_image'><img src='spirl_corl2020.gif' width="160" height="160"></div>
            <img src='spirl_start.jpeg' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function spirl_start() {
            document.getElementById('spirl_image').style.opacity = "1";
            }
            function spirl_stop() {
            document.getElementById('spirl_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/2010.11944">
            <papertitle>Accelerating Reinforcement Learning with Learned Skill Priors</papertitle>
            </a>
            <br>
            <strong>Karl Pertsch</strong>, <a target="_blank" href="https://youngwoon.github.io/">Youngwoon Lee</a>, <a target="_blank" href="https://viterbi-web.usc.edu/~limjj/">Joseph J. Lim</a><br>
            <i>Conference on Robot Learning (CoRL)</i>, 2020 (<span style="color: #ff0000">Plenary Talk, top 4%</span>)<br>
            <i>Workshop on Robot Learning @ NeurIPS</i>, 2020 (<span style="color: #ff0000">Best Paper Runner-up Award</span>)<br>
            <a target="_blank" href ="https://clvrai.github.io/spirl/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2010.11944">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/clvrai/spirl">code</a>
            <br>
          </p>
          <p> We jointly learn an embedding space of skills and a prior over skills. This skill prior tells us <b>when</b> to use <b>which</b> skill and guides learning on new tasks for effective skill transfer from large offline datasets.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="mopa_start()" onmouseover="mopa_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'mopa_image'><img src='mopa_corl2020.gif' width="160" height="160"></div>
            <img src='mopa_start.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function mopa_start() {
            document.getElementById('mopa_image').style.opacity = "1";
            }
            function mopa_stop() {
            document.getElementById('mopa_image').style.opacity = "0";
            }
            // mopa_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/2010.11940">
            <papertitle>Motion Planner Augmented Reinforcement Learning for Robot Manipulation in Obstructed Environments</papertitle>
            </a>
            <br>
            <a target="_blank" href="https://www.junjungoal.tech/">Jun Yamada</a>*, <a target="_blank" href="https://youngwoon.github.io/">Youngwoon Lee</a>*, <a target="_blank" href="https://www.gautamsalhotra.com/">Gautam Salhorta</a>, <strong>Karl Pertsch</strong>, <a target="_blank" href="https://mpflueger.github.io/">Max Pflueger</a>, <a target="_blank" href="http://robotics.usc.edu/~gaurav/">Gaurav S.Sukhatme</a>, <a target="_blank" href="https://viterbi-web.usc.edu/~limjj/">Joseph J. Lim</a>, <a target="_blank" href="http://www.peter-englert.net/">Peter Englert</a><br>
            <i>Conference on Robot Learning (CoRL)</i>, 2020<br>
            <a target="_blank" href ="https://clvrai.github.io/mopa-rl/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2010.11940">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/clvrai/mopa-rl">code</a>
            <br>
          </p>
          <p> Our approach augments model-free RL agents with motion planning capabilities, enabling them to solve long-horizon manipulation tasks in cluttered environments.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="gcp_start()" onmouseover="gcp_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'gcp_image'><img src='gcp_animate.gif' width="160" height="160"></div>
            <img src='gcp.jpeg' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function gcp_start() {
            document.getElementById('gcp_image').style.opacity = "1";
            }
            function gcp_stop() {
            document.getElementById('gcp_image').style.opacity = "0";
            }
            // gcp_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/2006.13205">
            <papertitle>Long-Horizon Visual Planning with Goal-Conditioned Hierarchical Predictors</papertitle>
            </a>
            <br>
            <strong>Karl Pertsch</strong>*, <a target="_blank" href="http://www.cis.upenn.edu/~oleh/">Oleh Rybkin</a>*, <a target="_blank" href="https://febert.github.io/">Frederik Ebert</a>, <a target="_blank" href="http://people.eecs.berkeley.edu/~cbfinn/">Chelsea Finn</a>, <a target="_blank" href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>, <a target="_blank" href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><br>
            <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2020<br>
            <a target="_blank" href ="https://orybkin.github.io/video-gcp/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2006.13205">arXiv</a>  /
            <a target="_blank" href ="https://youtu.be/axXx-x86IeY">video</a> / <a target="_blank" href ="https://github.com/orybkin/video-gcp">code</a>
            <br>
          </p>
          <p> We propose a hierarchical prediction model that predicts sequences by recursive infilling. We use this model to devise a hierarchical planning approach that allows to scale visual MPC to long-horizon tasks with hundreds of time steps.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="keyin_stop()" onmouseover="keyin_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'keyin_image'><img src='keyin.gif' width="160" height="160"></div>
            <img src='keyin.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function keyin_start() {
            document.getElementById('keyin_image').style.opacity = "1";
            }
            function keyin_stop() {
            document.getElementById('keyin_image').style.opacity = "0";
            }
            keyin_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/1904.05869">
            <papertitle>Keyframing the Future: Keyframe Discovery for Visual Prediction and Planning</papertitle>
            </a>
            <br>
            <strong>Karl Pertsch</strong>*, <a target="_blank" href="http://www.cis.upenn.edu/~oleh/">Oleh Rybkin</a>*, <a target="_blank" href="https://www.linkedin.com/in/yjy0625/">Jingyun Yang</a>, Shenghao Zhou, <a target="_blank" href="http://www.scs.ryerson.ca/kosta/">Kosta Derpanis</a>, <a target="_blank" href="http://www-bcf.usc.edu/~limjj/">Joseph Lim</a>, <a target="_blank" href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>, <a target="_blank" href="http://www.drewjaegle.com/">Andrew Jaegle</a><br>
            <i>Conference on Learning for Dynamics and Control</i>, 2020<br>
            <a target="_blank" href ="https://sites.google.com/view/keyin">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/1904.05869">arXiv</a>  /
            <a target="_blank" href ="https://youtu.be/e2hVV5FDKf8">video</a> / <a target="_blank" href ="poster_keyin.pdf">poster</a>
            <br>
          </p>
          <p> We propose a keyframe-based video prediction model that can unsupervisedly discover the moments of interesting change, the <i>keyframes</i>, in the data. We show that using the predicted keyframes as subgoals for planning improves performance on a simulated pushing task.</p>
          <p>
          <i> Hover over image (or tap the screen) to see the video.</i></p>
          </p>
          </td>
        </tr>

	      <tr  onmouseout="sensorimotor_stop()" onmouseover="sensorimotor_start()">
	        <td width="25%">
	          <div class="one">
	          <div class="two" id = 'sensorimotor_image'><img src='CLASP_after.gif' width="160" height="160"></div>
	          <img src='CLASP_before.png' width="160" height="160">
	          </div>
	          <script type="text/javascript">
	          function sensorimotor_start() {
	          document.getElementById('sensorimotor_image').style.opacity = "1";
	          }
	          function sensorimotor_stop() {
	          document.getElementById('sensorimotor_image').style.opacity = "0";
	          }
	          sensorimotor_stop()
	          </script>
	        </td>
	        <td width="75%" valign="top">
	        <p>
	        <p>
	          <a target="_blank" href="https://arxiv.org/abs/1806.09655">
	          <papertitle>Learning what you can do before doing anything</papertitle>
	          </a>
	          <br>
	          <a target="_blank" href="http://www.cis.upenn.edu/~oleh/">Oleh Rybkin</a>*, <strong>Karl Pertsch</strong>*,  <a target="_blank" href="http://www.scs.ryerson.ca/kosta/">Kosta Derpanis</a>, <a target="_blank" href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>, <a target="_blank" href="http://www.drewjaegle.com/">Andrew Jaegle</a><br>
	          <i>International Conference on Learning Representations (ICLR)</i>, 2019<br>
	          <a target="_blank" href ="https://daniilidis-group.github.io/learned_action_spaces/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/1806.09655">arXiv</a>  /
	          <a target="_blank" href ="https://daniilidis-group.github.io/learned_action_spaces/poster.pdf">poster</a>
	          <br>
	        </p>
	        <p> We learn an agent's action space from pure visual observations along with a predictive model. It can then be used to perform model predictive control, requiring orders of magnitude fewer action annotated videos.</p>
	        <p>
	        <i> Hover over image (or tap the screen) to see the video.</i></p>
	        </p>
	        </td>
	      </tr>

	      <tr>
            <td width="25%">
              <img src='object_pose.png' width="160" height="160">
            </td>
            <td valign="top" width="75%">
              <p>
                <a target="_blank" href="https://arxiv.org/abs/1712.01924">
                  <papertitle>iPose: Instance-Aware 6D Pose Estimation of Partly Occluded Objects</papertitle>
                </a>
                <br>
                <a target="_blank" href="https://scholar.google.com/citations?user=-oPXmWIAAAAJ&hl=en">Omid Hosseini Jafari</a>*, <a target="_blank" href="http://sivakm.github.io/">Siva Karthik Mustikovela</a>*, <strong>Karl Pertsch</strong>,  <a target="_blank" href="https://hci.iwr.uni-heidelberg.de/vislearn/people/eric-brachmann/">Eric Brachmann</a>, <a target="_blank" href="https://hci.iwr.uni-heidelberg.de/vislearn/people/carsten-rother/">Carsten Rother</a><br>
                <i>Asian Conference on Computer Vision (ACCV)</i>, 2018
                <br>
                <p></p>
                <p>Combining a CNN-based regression of dense on-object surface labeling with RANSAC-based pose fitting for accurate 6DoF pose estimation of texture-less objects under heavy occlusion.</p>
            </td>
          </tr>
        </table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	      <tr>
	        <td>
	        <br>
	        <p align="right">
	          <font size="2">
	          <strong>I borrowed this website layout from <a target="_blank" href="https://jonbarron.info/">here</a>!</strong>
		    </font>
	        </p>
	        </td>
	      </tr>
      	</table>

        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
