<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
	<!-- Global Site Tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112301535-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-112301535-1');
	</script>
	  <meta name=viewport content=“width=800”>
	  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
	  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    li:not(:last-child) {
        margin-bottom: -10px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }

    img {
      border-radius: 15px;
    }
  </style>
  <link rel="icon" type="image/png" href="psihijatar-footer-1.png">
  <title>Karl Pertsch</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Karl Pertsch</name>
              </p>
              <p>I am a postdoc at UC Berkeley and Stanford University, where I work with <a target="_blank" href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> and <a target="_blank" href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a> on deep learning, reinforcement learning and robotics.</p>

              <p>I completed my PhD at the University of Southern California (USC), working with <a target="_blank" href="http://www-bcf.usc.edu/~limjj/">Joseph Lim</a>. During my PhD, I was fortunate to intern at Meta AI and spend time as a student researcher at Google Brain with <a target="_blank" href="https://karolhausman.github.io/">Karol Hausman</a>. Before my PhD, I spent one year as a Fulbright Scholar at the University of Pennsylvania, working with <a target="_blank" href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>.
              </p>
              <p align=center>
                <a href="mailto:pertsch@usc.edu">Email</a> &nbsp/&nbsp
                <a target="_blank" href="https://twitter.com/KarlPertsch"> Twitter </a> &nbsp/&nbsp
                <a target="_blank" href="https://scholar.google.com/citations?view_op=list_works&hl=en&user=3oe0I0QAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="resume/resume_pertsch.pdf">CV</a> &nbsp/&nbsp
                <a target="_blank" href="https://www.linkedin.com/in/karlpertsch/"> LinkedIn </a>
              </p>
            </td>
            <td width="33%">
              <img src="profile_karl.jpg" width="250" height="250">
            </td>
          </tr>
        </table>

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	      <tr>
	        <td width="100%" valign="middle">
	          <heading>News</heading>
	          <p>
	            <ul>
                <li> [Oct 2022] Our work on  <a target="_blank" href="https://kpertsch.github.io/star">cross-domain imitation learning</a> got accepted to CoRL'22! </li> <br>
                <li> [Mar 2022] Two papers accepted to ICLR 2022: <a target="_blank" href="https://clvrai.com/tarp">Task-Induced Representation Learning</a> and <a target="_blank" href="https://clvrai.com/simpl">Skill-based Meta-Reinforcement Learning</a>! </li> <br>
	             <li> [Sept 2021] Our <a target="_blank" href="https://arxiv.org/abs/2107.10253">SkiLD paper</a> will be presented at CoRL 2021! </li> <br>
                <li> [Jul 2021] New <a target="_blank" href="https://arxiv.org/abs/2107.10253">preprint</a> on skill-based learning with demonstrations! </li> <br>
                <li> [Jun 2021] I presented our work on skill-based reinforcement & imitation learning in the <a target="_blank" href=https://www.seas.upenn.edu/~dineshj/pal/index.html">PAL Lab</a> at UPenn and in the <a target="_blank" href=http://svl.stanford.edu/">Stanford Vision & Learning Lab</a>. Check the <a target="_blank" href="https://drive.google.com/file/d/14xn9ojYfv8rSxVf5fPTixnkTpJRWUKKg/view?usp=sharing">Slides here</a>!</li><br>
                <li> [Dec 2020] Received the CoRL 2020 <span style="color: #ff0000">Best Paper Presentation Award</span> for our <a target="_blank" href="https://arxiv.org/abs/2010.11944">SPiRL</a> paper, check out the <a target="_blank" href="https://youtu.be/kZOcqFRj5NE?t=5119">talk recording</a>!</li><br>
	              <li> [Nov 2020] <a target="_blank" href="https://arxiv.org/abs/2010.11944">SPiRL</a> will be presented as a <span style="color: #ff0000">plenary talk</span> at CoRL & won the <span style="color: #ff0000">best paper runner-up award</span> at the robot learning workshop @ NeurIPS!</li><br>
                <li> [Oct 2020] Two papers accepted to CoRL 2020 (<a target="_blank" href="https://arxiv.org/abs/2010.11944">SPiRL</a> and <a target="_blank" href="https://arxiv.org/abs/2010.11940">MoPA-RL</a>)! </li> <br> -->
                <!-- <li> [Sept 2020] Our hierarchical prediction and planning paper was accepted to NeurIPS2020!</li> <br> -->
                <!-- <li> [Jun 2020] New <a target="_blank" href="https://arxiv.org/abs/2006.13205">preprint</a> on long-horizon visual planning using hierarchical prediction! </li> <br> -->
	              <!-- <li> [Apr 2020] Our work on keyframe-based prediction will be presented at L4DC'20!</li> <br> -->
                <!-- <li> [Apr 2019] New <a target="_blank" href="https://arxiv.org/abs/1904.05869">preprint</a> on keyframe-based video prediction! </li> <br> -->
                <!-- <li> [Apr 2019] Our work on discovering an agent's action space got accepted to ICLR19! </li> <br> -->
                <!--<li> [Dec 2018] I presented our work on unsupervised learning of agent's action spaces at the <a target="_blank" href="https://sites.google.com/view/infer2control-nips2018">Infer2Control workshop</a> at NeurIPS 2018 in Montreal. </li> <br> -->
                <!-- <br> -->
	              <!-- <li> [Aug 2017] Starting my one year Fulbright research stay in <a target="_blank" href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> group at UPenn. </li> <br> -->
	            <!-- </ul>  
	          </p>
	        </td>
	      </tr>
      	</table> -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                I'm interested in machine learning, reinforcement learning and robotics. At the moment, I am working on methods for using large datasets to facilitate the learning of complex, long-horizon robotic behaviors. Towards this goal I explore approaches that learn world models, representations of the environment or reusable skills from offline data and transfer them to new tasks.
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tr  onmouseout="octo_start()" onmouseover="octo_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'octo_image'><img src='resources/octo.gif' width="160" height="100"></div>
            <img src='resources/octo.jpeg' width="160" height="100" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function octo_start() {
            document.getElementById('octo_image').style.opacity = "1";
            }
            function octo_stop() {
            document.getElementById('octo_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://octo-models.github.io/">
            <papertitle>Octo: An Open-Source Generalist Robot Policy</papertitle>
            </a>
            <br>
            <a target="_blank" href="https://dibyaghosh.com/">Dibya Ghosh</a>*, <a target="_blank" href="https://homerwalke.com/">Homer Walke</a>*, <strong>Karl Pertsch</strong>*, <a target="_blank" href="https://kevin.black/">Kevin Black</a>*, <a target="_blank" href="https://www.oiermees.com/">Oier Mees</a>*, ..., <a target="_blank" href="https://dorsa.fyi/">Dorsa Sadigh</a>, <a target="_blank" href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, <a target="_blank" href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><br>
            <i>ArXiV</i>, 2023<br>
            <a target="_blank" href ="https://octo-models.github.io/">project page</a> / <a target="_blank" href ="https://octo-models.github.io/paper.pdf">tech report</a>  / 
            <a target="_blank" href ="https://github.com/octo-models/octo">code</a>
            <br>
          </p>
          <p> We introduce Octo, an open-source generalist policy, trained on 800k robot trajectories. Octo is a large, transformer-based diffusion policy that supports flexible task specification, observation and action spaces. It can control a diverse range of robots out of the box and supports efficient finetuning to new robot configurations. We release pre-trained checkpoints and our full training + finetuning pipelines.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="rtx_start()" onmouseover="rtx_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'rtx_image'><img src='resources/rtx_thumbnail.gif' width="160" height="160"></div>
            <img src='resources/rtx_thumbnail.jpeg' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function rtx_start() {
            document.getElementById('rtx_image').style.opacity = "1";
            }
            function rtx_stop() {
            document.getElementById('rtx_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://robotics-transformer-x.github.io/">
            <papertitle>Open X-Embodiment: Robotic Learning Datasets and RT-X Models</papertitle>
            </a>
            <br>
            Open X-Embodiment Collaboration<br>
            <i>ArXiV</i>, 2023<br>
            <a target="_blank" href ="https://robotics-transformer-x.github.io/">project page</a> / <a target="_blank" href ="https://robotics-transformer-x.github.io/paper.pdf">arXiv</a>  / 
            <a target="_blank" href ="https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?usp=sharing">dataset</a>
            <br>
          </p>
          <p> We introduce the Open X-Embodiment Dataset, the largest robot learning dataset to date with 1M+ real robot trajectories, spanning 22 robot embodiments. We train large, transformer-based policies on the dataset (RT-1-X, RT-2-X) and show that co-training with our diverse dataset substantially improves performance.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="star_start()" onmouseover="star_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'star_image'><img src='resources/star.gif' width="160" height="160"></div>
            <img src='resources/star.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function star_start() {
            document.getElementById('star_image').style.opacity = "1";
            }
            function star_stop() {
            document.getElementById('star_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://kpertsch.github.io/star">
            <papertitle>Cross-Domain Transfer via Semantic Skill Imitation</papertitle>
            </a>
            <br>
            <strong>Karl Pertsch</strong>, <a target="_blank" href="https://rutadesai.github.io/">Ruta Desai</a>, <a target="_blank" href="https://vikashplus.github.io/">Vikash Kumar</a>, <a target="_blank" href="https://fmeier.github.io/">Franziska Meier</a>, <a target="_blank" href="https://www.clvrai.com/">Joseph J. Lim</a>, <a target="_blank" href="https://faculty.cc.gatech.edu/~dbatra/">Dhruv Batra</a>, <a target="_blank" href="https://ai.facebook.com/people/akshara-rai/">Akshara Rai</a><br>
            <i>Conference on Robot Learning (CoRL)</i>, 2022<br>
            <a target="_blank" href ="https://kpertsch.github.io/star">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2212.07407">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/kpertsch/star">code</a>
            <br>
          </p>
          <p> We learn a semantic skill policy that enables cross-domain imitation: from robot to robot between different environments and even from human video to robot. We show that we can learn long-horizon robotic manipulation tasks in a simulated kitchen environment using only three minutes of human video, recorded in my kitchen with a GoPro strapped to my head.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="pato_start()" onmouseover="pato_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'pato_image'><img src='resources/pato.gif' width="160" height="160"></div>
            <img src='resources/pato.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function pato_start() {
            document.getElementById('pato_image').style.opacity = "1";
            }
            function pato_stop() {
            document.getElementById('pato_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://clvrai.github.io/pato">
            <papertitle>Assisted Teleoperation for Scalable Robot Data Collection</papertitle>
            </a>
            <br>
            <a target="_blank" href="https://shivindass.github.io">Shivin Dass</a>*, <strong>Karl Pertsch</strong>*, <a target="_blank" href="https://www.hejiazhang.me/">Hejia Zhang</a>, <a target="_blank" href="https://youngwoon.github.io">Youngwoon Lee</a>, <a target="_blank" href="https://www.clvrai.com/">Joseph J. Lim</a>, <a target="_blank" href="https://stefanosnikolaidis.net/">Stefanos Nikolaidis</a><br>
            <!-- <i>Conference on Robot Learning (CoRL)</i>, 2022<br> -->
            <a target="_blank" href ="https://clvrai.github.io/pato">project page</a> / <a target="_blank" href ="https://drive.google.com/file/d/1pqgBTju0p7-mUUGl1IYoh4gQ1q_qbL7m/view?usp=sharing">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/clvrai/pato">code</a>
            <br>
          </p>
          <p> We enable scalable robot data collection by assisting human teleoperators with a learned policy. Our approach estimates its uncertainty over future actions to determine when to request user input. In real world user studies we demonstrate that our system enables more efficient teleoperation with reduced mental load and up to four robots in parallel.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="tarp_start()" onmouseover="tarp_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'tarp_image'><img src='resources/tarp.png' width="160" height="160"></div>
            <img src='resources/tarp.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function tarp_start() {
            document.getElementById('tarp_image').style.opacity = "1";
            }
            function tarp_stop() {
            document.getElementById('tarp_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://clvrai.com/tarp">
            <papertitle>Task-Induced Representation Learning</papertitle>
            </a>
            <br>
            <a target="_blank" href="https://junjungoal.github.io/">Jun Yamada</a>, <strong>Karl Pertsch</strong>, <a target="_blank" href="https://anisha2102.github.io/">Anisha Gunjal</a>, <a target="_blank" href="https://viterbi-web.usc.edu/~limjj/">Joseph J. Lim</a><br>
            <i>International Conference on Learning Representations (ICLR)</i>, 2022<br>
            <a target="_blank" href ="https://clvrai.com/tarp">project page</a> / <a target="_blank" href ="https://openreview.net/forum?id=OzyXtIZAzFv">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/clvrai/tarp">code</a>
            <br>
          </p>
          <p> We evaluate the effectiveness of representation learning approaches on visually complex environments with substantial distractors. We compare common unsupervised representation learning approaches to task-induced representations, that leverage task information from prior tasks to learn what parts of the scene are important to model and what parts can be ignored.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="simpl_start()" onmouseover="simpl_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'simpl_image'><img src='resources/simpl_teaser.png' width="160" height="160"></div>
            <img src='resources/simpl_teaser.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function simpl_start() {
            document.getElementById('simpl_image').style.opacity = "1";
            }
            function simpl_stop() {
            document.getElementById('simpl_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://clvrai.com/simpl">
            <papertitle>Skill-based Meta-Reinforcement Learning</papertitle>
            </a>
            <br>
            <a target="_blank" href="https://namsan96.github.io/">Taewook Nam</a>, <a target="_blank" href="https://shaohua0116.github.io/">Shao-Hua Sun</a>, <strong>Karl Pertsch</strong>, <a target="_blank" href="http://www.sungjuhwang.com/">Sung Ju Hwang</a>, <a target="_blank" href="https://viterbi-web.usc.edu/~limjj/">Joseph J. Lim</a><br>
            <i>International Conference on Learning Representations (ICLR)</i>, 2022<br>
            <a target="_blank" href ="https://clvrai.com/simpl">project page</a> / <a target="_blank" href ="https://openreview.net/forum?id=jeLW-Fh9bV">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/namsan96/simpl">code</a>
            <br>
          </p>
          <p> We perform meta-RL on top of skills extracted from large task-agnostic offline datasets. By combining meta-training tasks with offline data we can meta-learn policies that can quickly learn new long-horizon, sparse reward tasks.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="skild_start()" onmouseover="skild_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'skild_image'><img src='skild_thumbnail.gif' width="160" height="160"></div>
            <img src='skild_start.jpeg' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function skild_start() {
            document.getElementById('skild_image').style.opacity = "1";
            }
            function skild_stop() {
            document.getElementById('skild_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/2107.10253">
            <papertitle>Demonstration-Guided Reinforcement Learning with Learned Skills</papertitle>
            </a>
            <br>
            <strong>Karl Pertsch</strong>, <a target="_blank" href="https://youngwoon.github.io/">Youngwoon Lee</a>, <a target="_blank" href="https://ventusyue.github.io/">Yue Wu</a>, <a target="_blank" href="https://viterbi-web.usc.edu/~limjj/">Joseph J. Lim</a><br>
            <i>Conference on Robot Learning (CoRL)</i>, 2021<br>
            <a target="_blank" href ="https://clvrai.github.io/skild/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2107.10253">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/clvrai/skild">code</a>
            <br>
          </p>
          <p> We follow long-horizon demonstrations by imitating the demonstrated skills instead of the primitive actions. By using skills learned from large, task-agnostic experience datasets for imitation, our approach SkiLD can seamlessly integrate task-agnostic data & demonstrations via a skill-based learning framework.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="spirl_start()" onmouseover="spirl_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'spirl_image'><img src='spirl_corl2020.gif' width="160" height="160"></div>
            <img src='spirl_start.jpeg' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function spirl_start() {
            document.getElementById('spirl_image').style.opacity = "1";
            }
            function spirl_stop() {
            document.getElementById('spirl_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/2010.11944">
            <papertitle>Accelerating Reinforcement Learning with Learned Skill Priors</papertitle>
            </a>
            <br>
            <strong>Karl Pertsch</strong>, <a target="_blank" href="https://youngwoon.github.io/">Youngwoon Lee</a>, <a target="_blank" href="https://viterbi-web.usc.edu/~limjj/">Joseph J. Lim</a><br>
            <i>Conference on Robot Learning (CoRL)</i>, 2020 (<span style="color: #ff0000">Plenary Talk, top 4%</span>)<br>
            <i>Workshop on Robot Learning @ NeurIPS</i>, 2020 (<span style="color: #ff0000">Best Paper Runner-up Award</span>)<br>
            <i>Deep RL Workshop @ NeurIPS</i>, 2020 (<span style="color: #ff0000">Oral</span>)<br>
            <a target="_blank" href ="https://clvrai.github.io/spirl/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2010.11944">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/clvrai/spirl">code</a>
            <br>
          </p>
          <p> We jointly learn an embedding space of skills and a prior over skills. This skill prior tells us <b>when</b> to use <b>which</b> skill and guides learning on new tasks for effective skill transfer from large offline datasets.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="mopa_start()" onmouseover="mopa_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'mopa_image'><img src='mopa_corl2020.gif' width="160" height="160"></div>
            <img src='mopa_start.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function mopa_start() {
            document.getElementById('mopa_image').style.opacity = "1";
            }
            function mopa_stop() {
            document.getElementById('mopa_image').style.opacity = "0";
            }
            // mopa_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/2010.11940">
            <papertitle>Motion Planner Augmented Reinforcement Learning for Robot Manipulation in Obstructed Environments</papertitle>
            </a>
            <br>
            <a target="_blank" href="https://www.junjungoal.tech/">Jun Yamada</a>*, <a target="_blank" href="https://youngwoon.github.io/">Youngwoon Lee</a>*, <a target="_blank" href="https://www.gautamsalhotra.com/">Gautam Salhorta</a>, <strong>Karl Pertsch</strong>, <a target="_blank" href="https://mpflueger.github.io/">Max Pflueger</a>, <a target="_blank" href="http://robotics.usc.edu/~gaurav/">Gaurav S.Sukhatme</a>, <a target="_blank" href="https://viterbi-web.usc.edu/~limjj/">Joseph J. Lim</a>, <a target="_blank" href="http://www.peter-englert.net/">Peter Englert</a><br>
            <i>Conference on Robot Learning (CoRL)</i>, 2020<br>
            <a target="_blank" href ="https://clvrai.github.io/mopa-rl/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2010.11940">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/clvrai/mopa-rl">code</a>
            <br>
          </p>
          <p> Our approach augments model-free RL agents with motion planning capabilities, enabling them to solve long-horizon manipulation tasks in cluttered environments.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="gcp_start()" onmouseover="gcp_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'gcp_image'><img src='gcp_animate.gif' width="160" height="160"></div>
            <img src='gcp.jpeg' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function gcp_start() {
            document.getElementById('gcp_image').style.opacity = "1";
            }
            function gcp_stop() {
            document.getElementById('gcp_image').style.opacity = "0";
            }
            // gcp_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/2006.13205">
            <papertitle>Long-Horizon Visual Planning with Goal-Conditioned Hierarchical Predictors</papertitle>
            </a>
            <br>
            <strong>Karl Pertsch</strong>*, <a target="_blank" href="http://www.cis.upenn.edu/~oleh/">Oleh Rybkin</a>*, <a target="_blank" href="https://febert.github.io/">Frederik Ebert</a>, <a target="_blank" href="http://people.eecs.berkeley.edu/~cbfinn/">Chelsea Finn</a>, <a target="_blank" href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>, <a target="_blank" href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><br>
            <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2020<br>
            <a target="_blank" href ="https://orybkin.github.io/video-gcp/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2006.13205">arXiv</a>  /
            <a target="_blank" href ="https://youtu.be/axXx-x86IeY">video</a> / <a target="_blank" href ="https://github.com/orybkin/video-gcp">code</a>
            <br>
          </p>
          <p> We propose a hierarchical prediction model that predicts sequences by recursive infilling. We use this model to devise a hierarchical planning approach that allows to scale visual MPC to long-horizon tasks with hundreds of time steps.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="keyin_stop()" onmouseover="keyin_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'keyin_image'><img src='keyin.gif' width="160" height="160"></div>
            <img src='keyin.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function keyin_start() {
            document.getElementById('keyin_image').style.opacity = "1";
            }
            function keyin_stop() {
            document.getElementById('keyin_image').style.opacity = "0";
            }
            keyin_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/1904.05869">
            <papertitle>Keyframing the Future: Keyframe Discovery for Visual Prediction and Planning</papertitle>
            </a>
            <br>
            <strong>Karl Pertsch</strong>*, <a target="_blank" href="http://www.cis.upenn.edu/~oleh/">Oleh Rybkin</a>*, <a target="_blank" href="https://www.linkedin.com/in/yjy0625/">Jingyun Yang</a>, Shenghao Zhou, <a target="_blank" href="http://www.scs.ryerson.ca/kosta/">Kosta Derpanis</a>, <a target="_blank" href="http://www-bcf.usc.edu/~limjj/">Joseph Lim</a>, <a target="_blank" href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>, <a target="_blank" href="http://www.drewjaegle.com/">Andrew Jaegle</a><br>
            <i>Conference on Learning for Dynamics and Control</i>, 2020<br>
            <a target="_blank" href ="https://sites.google.com/view/keyin">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/1904.05869">arXiv</a>  /
            <a target="_blank" href ="https://youtu.be/e2hVV5FDKf8">video</a> / <a target="_blank" href ="poster_keyin.pdf">poster</a>
            <br>
          </p>
          <p> We propose a keyframe-based video prediction model that can unsupervisedly discover the moments of interesting change, the <i>keyframes</i>, in the data. We show that using the predicted keyframes as subgoals for planning improves performance on a simulated pushing task.</p>
          <p>
          <i> Hover over image (or tap the screen) to see the video.</i></p>
          </p>
          </td>
        </tr>

	      <tr  onmouseout="sensorimotor_stop()" onmouseover="sensorimotor_start()">
	        <td width="25%">
	          <div class="one">
	          <div class="two" id = 'sensorimotor_image'><img src='CLASP_after.gif' width="160" height="160"></div>
	          <img src='CLASP_before.png' width="160" height="160">
	          </div>
	          <script type="text/javascript">
	          function sensorimotor_start() {
	          document.getElementById('sensorimotor_image').style.opacity = "1";
	          }
	          function sensorimotor_stop() {
	          document.getElementById('sensorimotor_image').style.opacity = "0";
	          }
	          sensorimotor_stop()
	          </script>
	        </td>
	        <td width="75%" valign="top">
	        <p>
	        <p>
	          <a target="_blank" href="https://arxiv.org/abs/1806.09655">
	          <papertitle>Learning what you can do before doing anything</papertitle>
	          </a>
	          <br>
	          <a target="_blank" href="http://www.cis.upenn.edu/~oleh/">Oleh Rybkin</a>*, <strong>Karl Pertsch</strong>*,  <a target="_blank" href="http://www.scs.ryerson.ca/kosta/">Kosta Derpanis</a>, <a target="_blank" href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>, <a target="_blank" href="http://www.drewjaegle.com/">Andrew Jaegle</a><br>
	          <i>International Conference on Learning Representations (ICLR)</i>, 2019<br>
	          <a target="_blank" href ="https://daniilidis-group.github.io/learned_action_spaces/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/1806.09655">arXiv</a>  /
	          <a target="_blank" href ="https://daniilidis-group.github.io/learned_action_spaces/poster.pdf">poster</a>
	          <br>
	        </p>
	        <p> We learn an agent's action space from pure visual observations along with a predictive model. It can then be used to perform model predictive control, requiring orders of magnitude fewer action annotated videos.</p>
	        <p>
	        <i> Hover over image (or tap the screen) to see the video.</i></p>
	        </p>
	        </td>
	      </tr>

	      <tr>
            <td width="25%">
              <img src='object_pose.png' width="160" height="160">
            </td>
            <td valign="top" width="75%">
              <p>
                <a target="_blank" href="https://arxiv.org/abs/1712.01924">
                  <papertitle>iPose: Instance-Aware 6D Pose Estimation of Partly Occluded Objects</papertitle>
                </a>
                <br>
                <a target="_blank" href="https://scholar.google.com/citations?user=-oPXmWIAAAAJ&hl=en">Omid Hosseini Jafari</a>*, <a target="_blank" href="http://sivakm.github.io/">Siva Karthik Mustikovela</a>*, <strong>Karl Pertsch</strong>,  <a target="_blank" href="https://hci.iwr.uni-heidelberg.de/vislearn/people/eric-brachmann/">Eric Brachmann</a>, <a target="_blank" href="https://hci.iwr.uni-heidelberg.de/vislearn/people/carsten-rother/">Carsten Rother</a><br>
                <i>Asian Conference on Computer Vision (ACCV)</i>, 2018
                <br>
                <p></p>
                <p>Combining a CNN-based regression of dense on-object surface labeling with RANSAC-based pose fitting for accurate 6DoF pose estimation of texture-less objects under heavy occlusion.</p>
            </td>
          </tr>
        </table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	      <tr>
	        <td>
	        <br>
	        <p align="right">
	          <font size="2">
	          <strong>I borrowed this website layout from <a target="_blank" href="https://jonbarron.info/">here</a>!</strong>
		    </font>
	        </p>
	        </td>
	      </tr>
      	</table>

        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
